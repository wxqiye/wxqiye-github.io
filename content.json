{"meta":{"title":"qiye","subtitle":"","description":"","author":"qiye","url":"https://wxqiye.github.io","root":"/"},"pages":[{"title":"about","date":"2022-04-23T11:56:25.000Z","updated":"2022-04-27T05:50:31.829Z","comments":true,"path":"about/index.html","permalink":"https://wxqiye.github.io/about/index.html","excerpt":"","text":"很高兴你来到这里！ 蒟蒻正在进行深度学习的学习… 本博客主要用来，纪念过去，以及记录现在。用来辅助自己当下的记忆，以后的复习，所以会写的非常简略！ 其他人看起来可能会很恼火，那就对啦（逃）。","author":"qiye"},{"title":"categories","date":"2022-04-23T11:52:56.000Z","updated":"2022-04-27T05:51:26.346Z","comments":true,"path":"categories/index.html","permalink":"https://wxqiye.github.io/categories/index.html","excerpt":"","text":""},{"title":"archives","date":"2022-04-23T11:54:29.000Z","updated":"2022-04-27T05:51:05.871Z","comments":true,"path":"archives/index.html","permalink":"https://wxqiye.github.io/archives/index.html","excerpt":"","text":""},{"title":"friends","date":"2022-04-23T11:55:23.000Z","updated":"2022-04-27T05:51:45.863Z","comments":true,"path":"friends/index.html","permalink":"https://wxqiye.github.io/friends/index.html","excerpt":"","text":""},{"title":"tags","date":"2022-04-23T11:37:56.000Z","updated":"2022-04-27T05:48:12.548Z","comments":true,"path":"tags/index.html","permalink":"https://wxqiye.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"激活函数在反向传播作用","slug":"激活函数在反向传播作用","date":"2022-05-15T05:54:24.000Z","updated":"2022-05-15T15:58:27.248Z","comments":true,"path":"2022/05/15/激活函数在反向传播作用/","link":"","permalink":"https://wxqiye.github.io/2022/05/15/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%9C%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BD%9C%E7%94%A8/","excerpt":"本文深入探讨激活函数，反向传播，激活函数在反向传播作用","text":"本文深入探讨激活函数，反向传播，激活函数在反向传播作用 我们都知道激活函数作用为增加模型非线性性。而反向传播，是用损失函数算对各个参数的偏导，再用梯度下降进行更新。那么激活函数在反向传播的作用是什么呢，要明晰这一点，我们需要理解具体过程。 在我看来，激活函数在反向传播的主要作用，是在反向传播算偏导时，多乘了一项，举例即： 多乘了 具体会在下文进行讲解。 而反向传播算法，计算误差项时每一层都要乘以本层激活函数的导数，而本层激活函数是非线性，导数也是非线性。 所以增加了函数的非线性性，并且反向传播也会跟依此算出这个非线性函数的偏导。 首先重新回顾激活函数跟反向传播的一些核心概念。 激活函数我们为什么需要激活函数？ 首先明晰神经网络的本质就是一个多层复合函数。 已知，线性函数无论嵌套多少层，他依然是线性，所以他的目标函数只可能是线性。我们无论对他进行怎样的梯度求导，因其目标函数为线性，最终也只能求得线性。 于是就有了激活函数，理论上，只要激活函数恰当，神经元个数足够多，使用3层（即包含一个隐藏层）的神经网络，就可以实现对于一切函数的逼近。数学定理为万能逼近原则。 有此定理，我们至少知道，使用激活函数后，他的目标函数，理论上是可以拟合成为任何我们需要的函数的。那就有用反向传播，梯度下降，进行逼近的可能性。 反向传播反向传播，就是用损失函数求每一个参数的偏导。具体计算可能较复杂，但理解起来较简单。 激活函数在反向传播的作用我们复现反向传播的具体过程： 对于参数w5可得 首先，我们明确，激活函数本身是没有参数可更新的。我们可以看到，反向传播算法计算误差项时每一层都要乘以本层激活函数的导数，在本例中即outo1对neto1的偏导。即： c为本层误差，z为参数，a为激活值。 若存在激活函数，有了非线性，我们可以得到如下偏导： 即 若不存在激活函数，为线性，我们可以得到如下偏导：","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://wxqiye.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://wxqiye.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"对卷积的理解","slug":"对卷积的理解","date":"2022-05-15T04:22:31.000Z","updated":"2022-05-15T16:04:42.488Z","comments":true,"path":"2022/05/15/对卷积的理解/","link":"","permalink":"https://wxqiye.github.io/2022/05/15/%E5%AF%B9%E5%8D%B7%E7%A7%AF%E7%9A%84%E7%90%86%E8%A7%A3/","excerpt":"本文主要为个人对卷积的理解，并将池化层与卷积进行了对比。（不一定对）","text":"本文主要为个人对卷积的理解，并将池化层与卷积进行了对比。（不一定对） 卷积的作用是提取特征，融合一定大小感受野的信息。神经网络可以学习卷积核来得到想要的输出。 卷积过程对图片进行卷积操作的过程为，对应像素点相乘再相加。输出新的特征图。如果是多通道的输入，最后还有多通道的对应相加。每一个三维卷积核只输出一个特征图。 若是想提取图片的多个特征，得到多个特征图，就需再加一维，有多少个三维卷积核，对应多少个输出通道，输出多少个特征图。 卷积本质通过以上我们可以得到，卷积本质是一种运算，一种信息融合，混合一定感受野像素点的运算.当然这个过程也可以看做是对特征的提取过滤筛选。不管哪一个解释，其本质都是通过梯度下降不断学习卷积里的参数，卷积为了得到有用信息逼近结果，参数会不断迭代，从而提取出更有利于结果的特征图。特征提取可以看做是另一种视角的特征融合，他们本质上是一样的。而这也可以叫做特征学习。 卷积与全连接对比 全连接的输出与所有输入相连，卷积只与部分输入相连，利用了图像的局部性，且减少了计算量。 全连接有多少个输出，就要有多少个w跟b，卷积相当于共享参数的全连接层，减少了模型复杂度。 卷积读取了空间信息，全连接不能。 卷积跟全连接的互相转换全连接变卷积：把一维的输入变成一个三维的东西，每一个输出要跟所有输入都有连接关系，所以卷积核的大小需要变成，宽跟高与输入一样大。即，每一个输出有自己的w跟b，且每一个输出都与所有输入有关。 卷积变全连接：把输入拉成一维，没有连接关系的乘个0。 实际运用通常，有多少个三维卷积核便会学到多少特征图。特征图再作为下一层的输入。如果说，已经到了最后一个卷积层。我们通常把每一个三维卷积核得到的特征图进行全局平均池化，再传给全连接进行分类，有多少类，全连接的输出维度便是几。 涉及的超参数有卷积核大小kernel_size 填充padding 卷积移动步长stride。 1*1的卷积层调整通道数。 相关池化层减小对位置的敏感度，分为最大池化层与平均池化层。最大池化层读取突出最强的那个模式信号，平均池化层比较柔和化。一般可以用来减小高宽。 与卷积层对比： 相同：都可以用来减小图片高宽。 不同： 卷积层主要进行特征提取，提取特征图。池化层主要是进行压缩，减小参数数量，最大池化层有突出最强模式信号的作用，平均池化层比较柔和，但他们都不算是提取特征图。 卷积层还可以改变通道数。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://wxqiye.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://wxqiye.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"使用resnet18进行猫狗分类","slug":"使用resnet18进行猫狗分类","date":"2022-05-15T03:59:25.000Z","updated":"2022-05-19T04:25:24.267Z","comments":true,"path":"2022/05/15/使用resnet18进行猫狗分类/","link":"","permalink":"https://wxqiye.github.io/2022/05/15/%E4%BD%BF%E7%94%A8resnet18%E8%BF%9B%E8%A1%8C%E7%8C%AB%E7%8B%97%E5%88%86%E7%B1%BB/","excerpt":"本文对resnet进行介绍，并使用resnet18进行猫狗分类，进行多项对比实验","text":"本文对resnet进行介绍，并使用resnet18进行猫狗分类，进行多项对比实验 resnet简介使用resnet18。 resnet18，18指带有权重的18层。 具体原理为，加更多层不一定能使精度更高，可能学偏，越学越远，精度更低。resnet，使下一层更复杂模型包含前面小模型作为子模型，这样复杂模型就严格比前面更大，学到的结果至少不会变差。本质是使用f(x)&#x3D;x+g(x)。这样进行梯度下降时，最坏的情况是g(x)完全没用甚至起反作用，但就算是这样，最差也能找到路径走回x。因为有这个加法的存在，上层模型的梯度可以有效的传到下层，所以有效解决了梯度消失的问题。 具体网络架构： 其中 1*1卷积层作用是如果通道数发生变换，则变换通道数使得与x与g(x)得以相加,使用最大池化层，多个高宽减半通道数加倍的卷积层，高宽不变通道数不变的卷积层，提取特征图。最后使用全局平均池化层提取单个通道的特征，拉伸成一维，作为全连接层的输入，全连接层的输出为类别数。所以最后一个全连接层我修改为了2。 损失函数为交叉熵。使用SGD，初始lr设为0.01，使用momentum，将前几轮的梯度叠加到当前计算中，按照0.9的比例衰减。使用weight_decay为5e-4防止过拟合。 使用MultiStepLR学习率下降策略，按设定的在相应的epoch里调整学习率，学习率调整倍数为0.2。 resnet 18精度与损失函数图： 在valid数据集上训练结果： 我们可以看出模型收敛效果较好。损失函数下降较快。在eopch为12左右模型就达到了收敛。但是训练精度明显大于测试精度。 在valid数据机上测得训练精度为0.89。 可以看出模型存在一定的过拟合现象，对于部分图片模型选择了记住他们，背住他们，导致泛化误差相对偏大。从而模型在背过答案的训练集上精度最高，在验证集与测试集上效果要差一点。 直接224 对比发现学到了噪音，效果差了很多。 使用dropout 精度与损失函数图： 在valid数据集上训练结果： 对比发现，在最后的全连接层，使用dropout加入噪音，并未使模型得到明显的改善，相反，模型的损失函数震荡过大，精度上升也偏慢。 测试集所得精度相似。 这一方面是因为batch_normal具有与dropout相似的正则化效果。有一种理论认为，bn层就是在模型里加入噪音。而batch_normal的每一小批次都减除不同的均值与方差（因为每一小批次的均值方差都不同），超参数也应lr较小，使得其超参数既在不断更新又比较稳定。所以正则化效果较好。 由此理论，再加入dropout，随机置0，再度增加噪音并无必要，查阅相关文献发现，bn层与dropout同时使用，甚至会使效果变差。 损失函数的震荡可能就是由于dropout的随机置零，导致其输出变化过大，从而在算损失函数的时候，变化过大。 且当前卷积神经网络已经普遍流行使用bn层，不使用dropout层，因为前者效果比后者好。 删去bn层 精度与损失函数图： 在valid数据集上训练结果： 对比发现，训练集验证集的模型精度变化不大，但是测试集的精度下降较多，且损失函数震荡较大。 这是因为bn通过减均值除方差，使他成为均值为0方差为1的分布，同时加入一定的线性变化，使得其分布在较稳定的一定范围内，且存在回到本来分布的途径，从而增加了恢复了一部分数据的原始表达能力。 这样做了之后，输入分布都较为稳定，学习也就相对稳定，上层网络也不需要一直调整很多的参数来适应下层不断改变的分布，从而加快了网络的训练速度。 所以加了bn层后，因输入分布都是较为稳定分布，学习较为稳定，所以损失函数下降的较为稳定，但是删去bn层后，损失函数就开始震荡不那么稳定了。 删去Residual connection 精度与损失函数图： 在valid数据集上训练结果： 通过对比可以知道，删去Residual connection层，对于模型测试精度，在前期，未训练到最优点时，并无太大影响。 但是如果增加epoch次数，由于没有使用残差结构，可能会出现模型精度达到最高点后，开始降低的情况。 减少resnet18层数删去b5层 精度与损失函数图： 在valid数据集上训练结果： 通过对比得知，模型在删去b5层后，在训练验证测试集上的精度有了明显的降低。说明减少深度后，模型对数据的拟合能力降低，需要进行更多轮的迭代，才可能达到理想效果。 不使用权重衰退就整体而言，对比加入正则化和未加入正则化的模型，训练输出的loss和Accuracy信息，我们可以发现，加入正则化后，loss下降的速度会变慢，准确率Accuracy的上升速度会变慢，并且未加入正则化模型的loss和Accuracy的浮动比较大（或者方差比较大），而加入正则化的模型训练loss和Accuracy，表现的比较平滑。并且随着正则化的权重lambda越大，表现的更加平滑。这其实就是正则化的对模型的惩罚作用，通过正则化可以使得模型表现的更加平滑，即通过正则化可以有效解决模型过拟合的问题。 精度与损失函数图： 在valid数据集上训练结果： 发现效果差不多，不过曲线上升的更为陡峭。 因为权重衰退是一种正则化，可以限制参数范围让曲线上升的更平滑。 但是发现最终效果差不多，说明模型的vc维是比较适合这个数据集的，模型的过拟合并不严重。 总结resnet模型出的效果很好，就是对比实验的结果，有时候有点难以解释，感觉不是很符合理论，需要再学习。 磕磕绊绊很多吧，具体不言表，但是相应的，收获也很多。 就这样啦！","categories":[{"name":"深度学习实践","slug":"深度学习实践","permalink":"https://wxqiye.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/"}],"tags":[{"name":"深度学习实践","slug":"深度学习实践","permalink":"https://wxqiye.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/"}]},{"title":"对于感知机的疑惑以及解答","slug":"对于感知机的疑惑以及解答","date":"2022-04-29T08:07:47.000Z","updated":"2022-05-15T15:59:02.393Z","comments":true,"path":"2022/04/29/对于感知机的疑惑以及解答/","link":"","permalink":"https://wxqiye.github.io/2022/04/29/%E5%AF%B9%E4%BA%8E%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E7%96%91%E6%83%91%E4%BB%A5%E5%8F%8A%E8%A7%A3%E7%AD%94/","excerpt":"学习不免产生很多疑问，如下：（只是我自己的思考，不能保证正确。） 感知机为什么是线性，多层感知机为什么是非线性。不是都有非线性激活函数吗。感知机到多层感知机，到底是因为引入了非线性激活函数，还是增加了层数。 早期感知机为什么只是加了一层，没有用其他非线性激活函数，就能处理异或问题。线性叠加多少层不还是线性吗。 到底是如何增加非线性的。","text":"学习不免产生很多疑问，如下：（只是我自己的思考，不能保证正确。） 感知机为什么是线性，多层感知机为什么是非线性。不是都有非线性激活函数吗。感知机到多层感知机，到底是因为引入了非线性激活函数，还是增加了层数。 早期感知机为什么只是加了一层，没有用其他非线性激活函数，就能处理异或问题。线性叠加多少层不还是线性吗。 到底是如何增加非线性的。 最本质的原因在于我对于线性跟非线性概念的理解不当。线性，非线性，本以为很简单的两个概念，成了拦路虎。 线性与非线性 辨析感知机与多层感知机的线性与非线性理清了线性跟非线性的概念，我们回看。 我们得到了标准，对于判断线性非线性的标准是看w。 感知机的辨析在早期的感知机，纵使他后面有一个非线性激活函数，但是并没有影响，他的每一个wi只影响对应的xi，所以他依然是线性的。 多层感知机的辨析但是，假如我们，再加一层，如图： 每一个x就被多个w影响了，并且因为有激活函数存在，无法把w*wi塌陷为一个参数w，x无法被单独拆分出去，即每个变量是无法与其它变量所独立开来，所以是非线性的！ 所以！ 解答问题一：感知机跟多层感知机，从线性到非线性， 最本质的区别，就是在于他多加了层数，使得线性变成了非线性！ 并没有刻意引入非线性激活函数，非线性激活函数，在最早期的感知机本来就有。 解答问题二：因为早期的感知机，本来就有非线性激活函数f(n)，所以，在后面加了一层就能处理异或问题，并不会造成塌陷。 解答问题三：非线性，需要激活函数，并且要多层，二者缺一不可。 激活函数保证不会塌陷成线性，多层保证一个x被多个w影响，共同作用保证了每个变量无法与其他变量独立开来。 如果没有激活函数会成为： (w1,w2这里是矩阵，为了便于理解,我就看成单一的w1,w2两个变量) 即会造成塌陷，w2*w1，完全可以重新组成一个w，他还是线性的。 如果是单层加激活函数就会成为： 每一个x依然只被一个w影响，所以是线性。 总结： 在感知机到多层感知机的变迁中，最本质在于增加了多层，将f(n)替换为sigmoid或者relu，仅仅是因为后者的拟合效果更好而已。理论上来说，他们都能无限逼近拟合任何非线性函数，只要层数足够多。 并且还有一点，在具体实现早期的感知机的时候，并没有用到激活函数，是用的 很巧妙的思路。","categories":[{"name":"深度学习疑问与解答","slug":"深度学习疑问与解答","permalink":"https://wxqiye.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%96%91%E9%97%AE%E4%B8%8E%E8%A7%A3%E7%AD%94/"}],"tags":[{"name":"深度学习疑问与解答","slug":"深度学习疑问与解答","permalink":"https://wxqiye.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%96%91%E9%97%AE%E4%B8%8E%E8%A7%A3%E7%AD%94/"}]},{"title":"softmax回归","slug":"softmax回归","date":"2022-04-29T05:29:42.000Z","updated":"2022-04-29T10:16:08.713Z","comments":true,"path":"2022/04/29/softmax回归/","link":"","permalink":"https://wxqiye.github.io/2022/04/29/softmax%E5%9B%9E%E5%BD%92/","excerpt":"本文深入理解softmax回归。 且将softmax与sigmoid，同放在最后一层进行了对比。","text":"本文深入理解softmax回归。 且将softmax与sigmoid，同放在最后一层进行了对比。 老生常谈，softmax回归不是回归，是分类。 具体如下： 区分二分跟softmax简单来讲，看最后的输出层，只有一个输出结果，就用f(n)亦或者其他二分的激活函数。 输出层有多个输出结果，用softmax,使其转变为了概率。 具体过程分类的标签采用独热编码，是这一类为1，不是为0： 则 采用交叉熵，量化损失 再求偏导，梯度下降即可得到开始训练。 改进因为softmax以e为底，求得的概率很难无限逼近1，且会丧失那些标签为0的数据的信息。所以可以将独热编码改为（0.9,0.1,0.1）。 思考假如不用softmax，用sigmoid会怎样。 虽然用梯度下降，还是在不断调参，预测值不断逼近标签值，但是，所有类的概率总和相加，不为1。 如果用sigmoid，像是在对每一个类分别单独判断，是不是这个类，是这个类的概率有多大的问题。 用softmax，才是判断是哪一个类的问题，有一个相对概率大小的概念。 总结softmax是一个很易于理解很巧妙的做法。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://wxqiye.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://wxqiye.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"卷积神经网络架构汇总","slug":"卷积神经网络架构汇总","date":"2022-04-29T04:55:20.000Z","updated":"2022-05-15T15:55:37.832Z","comments":true,"path":"2022/04/29/卷积神经网络架构汇总/","link":"","permalink":"https://wxqiye.github.io/2022/04/29/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E6%B1%87%E6%80%BB/","excerpt":"本文主要回顾各个经典的卷积神经网络，便于复习,resnet架构在resnet18进行猫狗分类里将的很详细，这里就不再赘述。","text":"本文主要回顾各个经典的卷积神经网络，便于复习,resnet架构在resnet18进行猫狗分类里将的很详细，这里就不再赘述。 Lenet 比较简单不多解释，中间有采用平均池化层，从卷积到全连接的时候，他直接选择了拉长，没有用全局平均池化。 最后使用softmax。 Alexnet本质是更深更大的Lenet 与lenet相比，采用了最大池化层，激活函数使用ReLU而不是sigmoid。 在全连接层的前两层使用dropout。 使用局部响应归一化，Local Response Normalization，使其中比较大的值变的相对更大，增加局部对比度。（不是很懂这个方法，下来再更新） VGG提出vgg块，相当于将Alexnet的这一块给规则化 成为 在同样计算开销下，使用3* 3更深的卷积比使用5* 5更浅的效果更好。所以vgg采用3*3。 vgg16架构。 NIN一卷积加两个1*1卷积，1 * 1卷积层不改变形状，大小，通道数，只是当做全连接层混合通道使用。 最后使用全局平均池化层，融合每一个通道的信息，甚至最后一层，安排合理的话，也不用全连接，将每一个通道融合就是输出了。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://wxqiye.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://wxqiye.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"day 8-20","slug":"day 8-20","date":"2022-04-27T05:27:54.000Z","updated":"2022-04-28T06:34:48.624Z","comments":true,"path":"2022/04/27/day 8-20/","link":"","permalink":"https://wxqiye.github.io/2022/04/27/day%208-20/","excerpt":"高中时候残存的一些博客","text":"高中时候残存的一些博客 \\1. 妹子 1 问题描述 万人迷皮皮轩收到了很多妹子的礼物，由于皮皮轩觉得每个妹子都不错，所以将她们礼物 的包装盒都好好保存，但长此以往皮皮轩的房间里都堆不下了，所以只能考虑将一些包装盒放 进其他包装盒里节省空间。 方便起见，我们不考虑包装盒的高度和厚度，只考虑包装盒的长宽。 一句话题意：给出两个矩形，问是否可以将一个矩形放在另一个矩形的内部（含边界），多 测。 2 输入格式 输入文件名为girls.in。 第一行，一个整数 n，表示数据组数。 对于下面的每一组数据： 第一行，四个整数 a1，b1，a2，b2 表示两个盒子的长宽。 3 输出格式 输出文件名为girls.out。 n 行，每行一个 ′′Y es′′ 或 ′′No′′（不含引号），分别表示其中一个盒子可以放到另一个盒子 中或两个盒子都不能放到另一个盒子中。 第一题第一个要注意的点是大对大，小对小，也就是可以旋转。 第二个点就是可以在侧着放，代码实现还在手打中。。 \\2. 旅程 1 背景 您曾经带领着我，穿过我的白天的拥挤不堪的旅程，而到达了我的黄昏的孤寂之境。在通 宵的寂静里，我等待着它的意义。 2 问题描述 神即将带领一些人去他们的孤寂之境，由于这个世界的不稳定，地点之间的有向道路会不 定期地毁坏，出于工作准备，神想知道在某些道路毁坏之后某两点之间的最短路。 就是给定一个有向图，现有两个操作，操作 1 是删除一条边（一条边可重复删除），操作 2 是询问两个点之间的最短路。 3 输入格式 输入文件名为journey.in。 第 1 行两个正整数 n, m，分别表示图的点数和操作数。 第 2 行至第 n + 1 行每行 n 个正整数，为图的邻接矩阵，第 i 行第 j 列的数表示点 i 和点 j 间距离，保证对角线为 0。 接下来 m 行每行三个正整数 c, x, y，c 表示操作种类，为 1 或 2，当 c &#x3D; 1 时表示删除 x 与 y 相连的边，当 c &#x3D; 2 时表示询问 x 到 y 的最短路，若不可达则输出 −1。 4 输出格式 输出文件名为journey.out。 输出若干行，每个 2 操作对应一行，答案为询问中 x 到 y 的最短路或 −1 第二题就是一个删边求最短路问题，比较巧妙的是回溯。先全部删完，用数组记录删掉的边，再回溯就行了。 123456789101112131415161718192021222324252627282930313233343536373839404142#include&lt;bits/stdc++.h&gt; using namespace std;const long long inf=0x3f3f3f3f;long long n,m,a[205][205],w[100005],op[100005],x[100005],y[100005],ans[100005],cnt;int main()&#123; scanf(&quot;%lld%lld&quot;,&amp;n,&amp;m); for(long long i=1;i&lt;=n;i++) for(long long j=1;j&lt;=n;j++) scanf(&quot;%lld&quot;,&amp;a[i][j]); for(long long i=1;i&lt;=m;i++) &#123; scanf(&quot;%lld%lld%lld&quot;,&amp;op[i],&amp;x[i],&amp;y[i]); if(op[i]==1)&#123; w[i]=a[x[i]][y[i]]; a[x[i]][y[i]]=inf; &#125; &#125; for(long long k=1;k&lt;=n;k++) for(long long i=1;i&lt;=n;i++) for(long long j=1;j&lt;=n;j++)&#123; a[i][j]=min(a[i][j],a[i][k]+a[k][j]); &#125; for(long long z=m;z&gt;=1;z--)&#123; if(op[z]==1)&#123; for(long long i=1;i&lt;=n;i++) for(long long j=1;j&lt;=n;j++)&#123; a[i][j]=min(a[i][j],a[i][x[z]]+a[y[z]][j]+w[z]); &#125; &#125; else ans[++cnt]=a[x[z]][y[z]]; &#125; for(long long i=cnt;i&gt;=1;i--)&#123; if(ans[i]&gt;=inf)&#123; printf(&quot;-1\\n&quot;); &#125; else printf(&quot;%lld\\n&quot;,ans[i]); &#125; return 0; &#125; \\3. 老大 1 问题描述 因为 OB 今年拿下 4 块金牌，学校赞助扩建劳模办公室为劳模办公室群，为了体现 OI 的 特色，办公室群被设计成了树形（n 个点 n − 1 条边的无向连通图），由于新建的办公室太大 以至于要将奖杯要分放在两个不同的地方以便同学们丢硬币进去开光，OB 想请你帮帮他看看 奖杯放在哪两个办公室使得在任意一个在劳模办公室做题的小朋友能最快地找到奖杯来开光。 一句话题意：给出一个 n 个点的树，在两个合适且不同的点放上奖杯，使得每个点到最近 的奖杯距离最大值最小。 2 输入格式 输入文件名为ob.in。 第一行，一个整数 n。 接下来的 n − 1 行，每行两个数 x y。 3 输出格式 输出文件名为ob.out。 三个数，分别表示最小的最大距离，两个奖杯的位置。 第三题代码里有很详细的注释。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include&lt;bits/stdc++.h&gt;using namespace std;const int M=2e5+5;int head[M],nxt[M&lt;&lt;1],to[M&lt;&lt;1],dis[M],l,r,mid,cnt,n,ans;void add(int f,int t)&#123;nxt[++cnt]=head[f],head[f]=cnt,to[cnt]=t;&#125;void dfs(int f,int v,int k)&#123; int mx=0,mn=M;//最大值，最小值。 for(int i=head[v];i;i=nxt[i]) if(to[i]!=f)//不是环 //这是从后到前的算法。 dfs(v,to[i],k),mx=max(mx,dis[to[i]]+1),mn=min(mn,dis[to[i]]+1); //i为这条边的编号，to[i]为他的终点，dis[to[i]]即为这一个点到最近奖杯的最大值 //深搜下一层，并在之后返回更新mx，mn， dis[v]=(mx+mn&lt;0?mn:mx);//相比较最大值最小值的绝对值谁更大，即为v的最大距离 if(dis[v]&gt;=k) dis[v]=-k-1,++ans; //这里需要结合前面的 dis[to[i]]+1。前面提到，这个算法是从后到前，后面的都已经大于等于k了，前面的就更不用说了。 //但这里并没有做什么特殊处理，遇到前面以这个v为终点时，那时候的dis[v]要么出现另一个。。。。。 //分析了这么久，好吧，其实是这样的，只是为了迎合 dis[v]=(mx+mn&lt;0?mn:mx);这个语句来判断dis。 //从他的最后一个点开始，依次向前，那时候的mn，mx都是很普通的，且没有负数,dis正常的保存最大值。 //直到找到第一个dis[v]大于等于k的点，累积进去，将这个值赋为-k-1。 //之后凡是以这个v为终点的边，mn第一次为-k, 此时需要ma为k才可以。 //如果这一次找到了ma除这一条边以外的k，ok，继续累加答案然后成为-k-1，如果没有，就成为-k，下一次需要k-1来解锁 //所以，意义何在？？？ //抽象一点想，这是两个奖杯。需要的是两个所有点与他们最近的那个最大相距k的奖杯，。 //最终结论： //把if(dis[v]&gt;=k) dis[v]=-k-1,++ans;看成放下奖杯。找到了这个点后，后面的不能依靠前面的直接达到k，因为是与最近的奖杯的距离最大值。 //所以ans统计的是放上的奖杯数？ //又错，是分段数。两个奖杯将所有分成至少三段。 //模拟一下，这里到这里，k个了，嗯，好，分一段，ans++，这里到这里，k个了，嗯，又分一段，这里到这里，嗯？没有k个？？？ //不慌，将k缩小试试，这里不能直接否定，因为第三段不一定需要达到k个。 //如果可以分成三段或者超过三段，就扩大试试，反正就是在把第三段分成小于k或者等于k之间徘徊 //还有一个问题，反着做呢？？？ //因为是后面k个，然后从后往前，k个k个的分，所以，所以？？？ //因为是树，所以后往前，从叶子到根点，一定最优。 &#125;bool check(int k) &#123;dfs(ans=0,1,k);//起点，终点，每一个点到最近奖杯距离的最大值。又需要所有的情况中最小的那种情况，需要有至少两个大于等于k //所以枚举这个最大值，如果如果有至少两个点满足情况，ok换，没有，ok，换。 if(dis[1]&gt;=0)++ans;return ans&gt;2;&#125;void in()&#123;scanf(&quot;%d&quot;,&amp;n);// for(int i=1,a,b;i&lt;n;++i)//scanf(&quot;%d%d&quot;,&amp;a,&amp;b),add(a,b),add(b,a);//&#125;void ac()&#123;for(l=0,r=n;l^r;)//l^r是终止条件，相同就终止。 mid=l+r&gt;&gt;1,check(mid)?l=mid+1:r=mid;//二分。 printf(&quot;%d&quot;,l);&#125;int main()&#123;in(),ac();&#125;","categories":[{"name":"高中竞赛回忆","slug":"高中竞赛回忆","permalink":"https://wxqiye.github.io/categories/%E9%AB%98%E4%B8%AD%E7%AB%9E%E8%B5%9B%E5%9B%9E%E5%BF%86/"}],"tags":[{"name":"高中竞赛回忆","slug":"高中竞赛回忆","permalink":"https://wxqiye.github.io/tags/%E9%AB%98%E4%B8%AD%E7%AB%9E%E8%B5%9B%E5%9B%9E%E5%BF%86/"}]},{"title":"斗地主","slug":"斗地主","date":"2022-04-27T05:26:45.000Z","updated":"2022-04-28T06:34:37.187Z","comments":true,"path":"2022/04/27/斗地主/","link":"","permalink":"https://wxqiye.github.io/2022/04/27/%E6%96%97%E5%9C%B0%E4%B8%BB/","excerpt":"高中时候残存的一些博客","text":"高中时候残存的一些博客 \\3. 斗地主（landlords.cpp&#x2F;c&#x2F;pas)【问题描述】牛牛最近迷上了一种叫斗地主的扑克游戏。斗地主是一种使用黑桃、红心、梅花、方片的 A 到 K 加上大小王的共 54 张牌来进行的扑克牌游戏。在斗地主中，牌的大小关系根据牌的数码表示如下：3&lt;4&lt;5&lt;6&lt;7&lt;8&lt;9&lt;10&lt;J&lt;Q&lt;K&lt;A&lt;2&lt;小王&lt;大王，而花色并不对牌的大小产生影响。每一局游戏中，一副手牌由 n 张牌组成。游戏者每次可以根据规定的牌型进行出牌，首先打光自己的手牌一方取得游戏的胜利。现在，牛牛只想知道，对于自己的若干组手牌，分别最少需要多少次出牌可以将它们打光。请你帮他解决这个问题。需要注意的是，本题中游戏者每次可以出手的牌型与一般的斗地主相似而略有不同。具体规则如下：牌型 牌型说明 牌型举例照片火箭 即双王（双鬼牌）。炸弹 四张同点牌。如四个 A。单张牌 单张牌，比如 3。对子牌 两张码数相同的牌。三张牌 三张码数相同的牌。三带一 三张码数相同的牌 + 一张单牌。例如：三张 3+单 4三带二 三张码数相同的牌 + 一对牌。例如：三张 3+对 4单顺子 五张或更多码数连续的单牌（不包括 2点和双王）例如：单 7+单 8+单 9+单 10+单 J。另外，在顺牌（单顺子、双顺子、三顺子）中，牌的花色不要求相同。双顺子 三对或更多码数连续的对牌（不包括 2点和双王）。例如：对 3+对 4+对 5。三顺子 二个或更多码数连续的三张牌（不能包括 2 点和双王）。例如：三张 3+三张 4+三张 5。四带二 四张码数相同的牌+任意两张单牌（或任意两对牌）例如：四张 5+单 3+单 8 或 四张 4+对 5+对 7【输入格式】全国信息学奥林匹克联赛（NOIP2015）复赛 提高组 day1第 5 页共 6 页输入文件名为 landlords.in。第一行包含用空格隔开的 2 个正整数 𝑇, 𝑛 ，表示手牌的组数以及每组手牌的张数。接下来 𝑇 组数据，每组数据 𝑛 行，每行一个非负整数对 𝑎𝑖, 𝑏𝑖，表示一张牌，其中 𝑎𝑖表示牌的数码，𝑏𝑖 表示牌的花色，中间用空格隔开。特别的，我们用 1 来表示数码 A，11 表示数码 J，12 表示数码 Q，13 表示数码 K；黑桃、红心、梅花、方片分别用 1-4 来表示；小王的表示方法为 0 1，大王的表示方法为 0 2。【输出格式】输出文件名为 landlords.out。共 T 行，每行一个整数，表示打光第𝑖组手牌的最少次数。【输入输出样例 1】landlords.in landlords.out1 87 48 49 110 411 15 11 41 13见选手目录下的 landlords&#x2F;landlords1.in 与 landlords&#x2F;landlords1.ans。【输入输出样例 1 说明】共有 1 组手牌，包含 8 张牌：方片 7，方片 8，黑桃 9，方片 10，黑桃 J，黑桃 5，方片 A 以及黑桃 A。可以通过打单顺子（方片 7，方片 8，黑桃 9，方片 10，黑桃 J），单张牌（黑桃 5）以及对子牌（黑桃 A 以及方片 A）在 3 次内打光。【输入输出样例 2】landlords.in landlords.out1 1712 34 32 35 410 23 312 20 11 310 16 212 111 36全国信息学奥林匹克联赛（NOIP2015）复赛 提高组 day1第 6 页共 6 页5 212 42 27 2见选手目录下的 landlords&#x2F;landlords2.in 与 landlords&#x2F;landlords2.ans。【样例输入输出 3】见选手目录下的 landlords&#x2F;landlords3.in 与 landlords&#x2F;landlords3.ans。【数据规模与约定】对于不同的测试点，我们约定手牌组数 𝑇 与张数 𝑛 的规模如下：测试点编号 𝑇 𝑛 测试点编号 𝑇 𝑛1 100 2 11 100 142 100 2 12 100 153 100 3 13 10 164 100 3 14 10 175 100 4 15 10 186 100 4 16 10 197 100 10 17 10 208 100 11 18 10 219 100 12 19 10 2210 100 13 20 10 23数据保证：所有的手牌都是随机生成的。 题解：狠了狠心还是自己手打了一遍。 这个题的思路并不复杂。 我的错主要在于分情况不清晰。 应该分为顺子，带牌，然后单独。（单独出牌直接判断是否有牌，因为1,2,3,4都可以直接打出去的！！！））。 然后注意一些细节，比如a比k大，a应该在k之后什么的。","categories":[{"name":"高中竞赛回忆","slug":"高中竞赛回忆","permalink":"https://wxqiye.github.io/categories/%E9%AB%98%E4%B8%AD%E7%AB%9E%E8%B5%9B%E5%9B%9E%E5%BF%86/"}],"tags":[{"name":"高中竞赛回忆","slug":"高中竞赛回忆","permalink":"https://wxqiye.github.io/tags/%E9%AB%98%E4%B8%AD%E7%AB%9E%E8%B5%9B%E5%9B%9E%E5%BF%86/"}]},{"title":"信息传递","slug":"信息传递","date":"2022-04-27T05:26:10.000Z","updated":"2022-04-28T06:34:25.914Z","comments":true,"path":"2022/04/27/信息传递/","link":"","permalink":"https://wxqiye.github.io/2022/04/27/%E4%BF%A1%E6%81%AF%E4%BC%A0%E9%80%92/","excerpt":"高中时候残存的一些博客","text":"高中时候残存的一些博客 题目描述有 nn 个同学（编号为 11 到 nn ）正在玩一个信息传递的游戏。在游戏里每人都有一个固定的信息传递对象，其中，编号为 ii 的同学的信息传递对象是编号为 T_iTi 的同学。 游戏开始时，每人都只知道自己的生日。之后每一轮中，所有人会同时将自己当前所知的生日信息告诉各自的信息传递对象（注意：可能有人可以从若干人那里获取信息， 但是每人只会把信息告诉一个人，即自己的信息传递对象）。当有人从别人口中得知自 己的生日时，游戏结束。请问该游戏一共可以进行几轮？ 输入输出格式输入格式： 共22行。 第11行包含1个正整数 nn ，表示 nn 个人。 第22行包含 nn 个用空格隔开的正整数 T_1,T_2,\\cdots\\cdots,T_nT1,T2,⋯⋯,Tn ，其中第 ii 个整数 T_iTi 表示编号为 ii 的同学的信息传递对象是编号为 T_iTi 的同学， T_i \\leq nTi≤n 且 T_i \\neq iTi≠i 。 输出格式： 11个整数，表示游戏一共可以进行多少轮。 输入输出样例输入样例#1： 复制 1252 4 2 3 1 输出样例#1： 复制 13 说明样例1解释 游戏的流程如图所示。当进行完第33 轮游戏后， 44号玩家会听到 22 号玩家告诉他自己的生日，所以答案为 33。当然，第 33 轮游戏后，22号玩家、 33 号玩家都能从自己的消息来源得知自己的生日，同样符合游戏结束的条件。 对于 30%30%的数据， n ≤ 200n≤200； 对于 60%60%的数据， n ≤ 2500n≤2500； 对于100%100%的数据， n ≤ 200000n≤200000。 1234567891011121314151617181920212223242526272829303132333435363738394041424344#include&lt;bits/stdc++.h&gt;using namespace std;int now=1,n,t[1000001],signn[1000001],ans=0x3f3f3f,d[1000001];int main()&#123; scanf(&quot;%d&quot;,&amp;n); for(int i=1;i&lt;=n;i++) &#123; scanf(&quot;%d&quot;,&amp;t[i]); &#125; for(int i=1;i&lt;=n;i++) &#123; int k=i,s=0; if(!signn[k]) &#123; while(1) &#123; if(signn[k]&amp;&amp;signn[k]!=now) &#123; break; &#125; else if(signn[k]==now) &#123; s++; if(ans&gt;s-d[k]) ans=s-d[k]; break; &#125; else &#123; signn[k]=now; s++; d[k]=s; k=t[k]; &#125; &#125; now++; &#125; &#125; printf(&quot;%d&quot;,ans); return 0;&#125; 题解：根据样例画图可以清晰的看到传递到原点的可以构成一个环。我们用now表示每一次的搜索。从没有搜索过的点开始。搜的时候注意用次序找每个环的值单纯用s累计算出来的不一定只有环，这个环可能还会拖着一截数据，s表示的是这全部，所以不行。","categories":[{"name":"高中竞赛回忆","slug":"高中竞赛回忆","permalink":"https://wxqiye.github.io/categories/%E9%AB%98%E4%B8%AD%E7%AB%9E%E8%B5%9B%E5%9B%9E%E5%BF%86/"}],"tags":[{"name":"高中竞赛回忆","slug":"高中竞赛回忆","permalink":"https://wxqiye.github.io/tags/%E9%AB%98%E4%B8%AD%E7%AB%9E%E8%B5%9B%E5%9B%9E%E5%BF%86/"}]},{"title":"神奇的幻方","slug":"神奇的幻方","date":"2022-04-27T05:25:29.000Z","updated":"2022-04-28T06:34:31.037Z","comments":true,"path":"2022/04/27/神奇的幻方/","link":"","permalink":"https://wxqiye.github.io/2022/04/27/%E7%A5%9E%E5%A5%87%E7%9A%84%E5%B9%BB%E6%96%B9/","excerpt":"高中时候残存的一些博客","text":"高中时候残存的一些博客 \\幻方是一种很神奇的 N*NN∗N 矩阵：它由数字 1,2,3,\\cdots \\cdots ,N \\times N1,2,3,⋯⋯,N×N 构成，且每行、每列及两条对角线上的数字之和都相同。 当 NN 为奇数时，我们可以通过下方法构建一个幻方： 首先将 11 写在第一行的中间。 之后，按如下方式从小到大依次填写每个数 K (K&#x3D;2,3,\\cdots,N \\times N)K(K&#x3D;2,3,⋯,N×N) ： 若 (K-1)(K−1) 在第一行但不在最后一列，则将 KK 填在最后一行， (K-1)(K−1) 所在列的右一列； 若 (K-1)(K−1) 在最后一列但不在第一行，则将 KK 填在第一列， (K-1)(K−1) 所在行的上一行； 若 (K-1)(K−1) 在第一行最后一列，则将 KK 填在 (K-1)(K−1) 的正下方； 若 (K-1)(K−1) 既不在第一行，也最后一列，如果 (K-1)(K−1) 的右上方还未填数，则将 KK 填在 (K-1)(K−1) 的右上方，否则将 LL 填在 (K-1)(K−1) 的正下方。 现给定 NN ，请按上述方法构造 N \\times NN×N 的幻方。 输入输出格式输入格式： 一个正整数 NN ，即幻方的大小。 输出格式： 共 NN 行 ，每行 NN 个整数，即按上述方法构造出的 N \\times NN×N 的幻方，相邻两个整数之间用单空格隔开。 12345678910111213141516171819202122232425262728293031323334353637383940414243#include&lt;bits/stdc++.h&gt;using namespace std;int n,a[40][40],x,y;int main()&#123; //freopen(&quot;magic.in&quot;,&quot;r&quot;,stdin); //freopen(&quot;magic.out&quot;,&quot;w&quot;,stdout); scanf(&quot;%d&quot;,&amp;n); a[1][n/2+1]=1; x=1,y=n/2+1; for(int i=2;i&lt;=n*n;i++) &#123; if(x==1&amp;&amp;y!=n) x=n,y+=1; else if(x!=1&amp;y==n) x-=1,y=1; else if(x==1&amp;&amp;y==n) x+=1; else if(x!=1&amp;&amp;y!=n) &#123; if(a[x-1][y+1]==0) x-=1,y+=1; else x+=1; &#125; a[x][y]=i; &#125; for(int i=1;i&lt;=n;i++) &#123; int k=0; for(int j=1;j&lt;=n;j++) &#123; if(k==0) &#123; printf(&quot;%d&quot;,a[i][j]); k=1; &#125; else printf(&quot; %d&quot;,a[i][j]); &#125; printf(&quot;\\n&quot;); &#125; return 0;&#125; 题解： 一道简单的模拟题，按照要求水一下就可以了，轻松愉快！！！","categories":[{"name":"高中竞赛回忆","slug":"高中竞赛回忆","permalink":"https://wxqiye.github.io/categories/%E9%AB%98%E4%B8%AD%E7%AB%9E%E8%B5%9B%E5%9B%9E%E5%BF%86/"}],"tags":[{"name":"高中竞赛回忆","slug":"高中竞赛回忆","permalink":"https://wxqiye.github.io/tags/%E9%AB%98%E4%B8%AD%E7%AB%9E%E8%B5%9B%E5%9B%9E%E5%BF%86/"}]},{"title":"引水入城","slug":"引水入城","date":"2022-04-27T05:19:54.000Z","updated":"2022-04-28T06:21:43.776Z","comments":true,"path":"2022/04/27/引水入城/","link":"","permalink":"https://wxqiye.github.io/2022/04/27/%E5%BC%95%E6%B0%B4%E5%85%A5%E5%9F%8E/","excerpt":"高中时候残存的一些博客","text":"高中时候残存的一些博客 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include&lt;bits/stdc++.h&gt;using namespace std;int n,m,sum,cnt;int max1,a[501][501],vis[501][501],l[501][501],r[501][501];int fx[4][2]=&#123;0,1,1,0,0,-1,-1,0&#125;;void dfs(int z,int y)&#123; vis[z][y]=1;//记录 for(int i=0;i&lt;=3;i++)//路线选择 &#123; int z1,y1; z1=z+fx[i][0]; y1=y+fx[i][1]; if(z1&lt;1||y1&lt;1||z1&gt;n||y1&gt;m||a[z][y]&lt;=a[z1][y1])//超出边界跳过 小于等于的跳过 continue;if(!vis[z1][y1])//没拜访过的才深搜。这里有点关键 dfs(z1,y1);l[z][y]=min(l[z][y],l[z1][y1]);//找最左边。r[z][y]=max(r[z][y],r[z1][y1]);//找最右边。 &#125;&#125;int main()&#123; scanf(&quot;%d%d&quot;,&amp;n,&amp;m); for(int i=1;i&lt;=n;i++) for(int j=1;j&lt;=m;j++) &#123; scanf(&quot;%d&quot;,&amp;a[i][j]); &#125; memset(l,0x3f,sizeof(l));//初始化为极大值，因为要在之后求到达最后一排相连的那么多数种最左边的 for(int i=1;i&lt;=m;i++) l[n][i]=r[n][i]=i; for(int i=1;i&lt;=m;i++) if(!vis[1][i])//没拜访过的就深搜。 dfs(1,i);for(int i=1;i&lt;=m;i++)&#123; if(vis[n][i]==0) &#123; cnt++;//有没拜访过的就标记并算出没拜访过的个数。 &#125;&#125;if(cnt!=0)&#123; puts(&quot;0&quot;); printf(&quot;%d&quot;,cnt);//为0的时候还好，输出，还好。 &#125;else&#123; int left=1;//最左为1？关键来了！！！ while(left&lt;=m) &#123; int max1=0; for(int i=1;i&lt;=m;i++) if(l[1][i]&lt;=left)//第一行1到m所能到的最后一行的左边边界的情况下找最右边界。自己画图想象吧。 max1=max(max1,r[1][i]); //左边界更新 sum++; left=max1+1;//不断找直到右边界达到m，即左边界在这一次更新会刚好超出。 &#125; puts(&quot;1&quot;); printf(&quot;%d&quot;,sum);//suo ga &#125; return 0;//本题关键点：在深搜的时候如果是已经拜访过的不能直接任性跳过，需要将这个点能到达的最左最右与x，y比较更新，否则wa。自己举例子理解吧，很简单的 &#125; 考试时:拿到这道题我就知道我做过的。可是我还是不会做，一直找不到完美的dfs的方案。我自己试着改变数据发现我编的代码对它的正确输出结果无能为力。那种感觉就是你明明检查出错误了，可你找不到正确的方法，想了好久，无奈。 考试后:才发现这道题我虽然做过的，但在很久以前就没改出来。具体思路写在代码注释里了，下面提一下我在自行编译的时候犯得小错。 在我写的深搜程序里，是没有访问过的才深搜。这点没问题。但没有访问过的不代表你不需要更新上一个点的所能到达最后一排的最左端和最右端所做的标记。仅仅只是不需要深搜而已，因为已经搜过了，当你到达这个点是已经搜过的点的时候，那么你之后的路径必然是和这个点已经走过的路径完全是重合的。但还是要更新数据。","categories":[{"name":"高中竞赛回忆","slug":"高中竞赛回忆","permalink":"https://wxqiye.github.io/categories/%E9%AB%98%E4%B8%AD%E7%AB%9E%E8%B5%9B%E5%9B%9E%E5%BF%86/"}],"tags":[{"name":"高中竞赛回忆","slug":"高中竞赛回忆","permalink":"https://wxqiye.github.io/tags/%E9%AB%98%E4%B8%AD%E7%AB%9E%E8%B5%9B%E5%9B%9E%E5%BF%86/"}]},{"title":"感知机","slug":"感知机","date":"2022-04-23T12:19:08.000Z","updated":"2022-04-29T08:17:57.732Z","comments":true,"path":"2022/04/23/感知机/","link":"","permalink":"https://wxqiye.github.io/2022/04/23/%E6%84%9F%E7%9F%A5%E6%9C%BA/","excerpt":"本文深入探讨什么是感知机，早期感知机的缺陷，以及缺陷的解决。以便于自己今后的复习。","text":"本文深入探讨什么是感知机，早期感知机的缺陷，以及缺陷的解决。以便于自己今后的复习。 什么是感知机早期感知机就是一个二分类的工具 类比 本质便是一条线性的线去拟合数据，将所有数据的点，通过梯度下降的方式，到最后，完全正确的分在线的上下两侧。 如果x只有一个便是二维，如下： x有两个便是三维，便是一个线性的平面去二分数据，如下： 其余的更高维情况我们可以依次类推。 我们回过来再看： 具体原因如下： 把点代入进那个式子，在上方的便大于0，在下方的便小于0。是不是很巧妙。 总结：感知机给所有分类问题一个统一的模板，剩下的就是使用梯度下降调参w。使线性的线（平面等）能够将所有数据二分。 激活函数式子比较简单这里不再赘述。 缺陷计算机很厉害，但是最本质无非就是与，或，非，异或这些逻辑运算。早期的感知机，只能实现与或非，无法实现异或等线性不可分的运算，无法拟合非线性的数据。 解决异或通过与或非的组合，得到解决。 其本质就是多加一层，类似于映射，把线性不可分的情况，转化为了最下面那个图，变得线性可分。 解决办法还有 升维： 以及把非线性的数据映射成线性的数据：","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://wxqiye.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://wxqiye.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"参数优化与正则化","slug":"参数优化与正则化","date":"2022-04-23T12:13:08.000Z","updated":"2022-05-18T08:57:44.043Z","comments":true,"path":"2022/04/23/参数优化与正则化/","link":"","permalink":"https://wxqiye.github.io/2022/04/23/%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/","excerpt":"","text":"参数优化SGD随机取一组batch_size数据的平均损失进行梯度下降，作用：减小震荡。 动量法 分解为w1,w2两个方向的分量，累加这些梯度的两个分量，一个方向的被抵消，如图：减少了震荡。 adagrad 刚开始梯度很大，后来梯度变小，但adagrad很容易过分累积梯度从而提前停止更新。 adam 本质就是自己设置的学习率自衰减策略。 正则化定义(不说人话)凡是可以减少泛化误差，而不是减少训练误差的方法，都可以叫做正则化。 即，凡是能减少过拟合的方法，都可以叫做正则化。 基本的概念理解为什么参数大小，会影响在测试集验证集下的误差明确我们求最小损失函数时的最优参数值不是唯一的，可以有很多组解。 所以就算损失函数相同，每一组最优解的w与b数值上很可能相差很大。 所以这个最优参数值的求得的大小很依赖于我们参数初始值的设定。 若是求得的最优解，数值本身比较大，在训练集上无影响，但是在测试集验证集上相乘，泛化误差，噪音，也会相应的放大。 所以想到控制参数大小，减小误差。 L1与L2正则化的对比这是L1正则化 这是L2正则化 首先，L1与L2正则化，其本质上是一样的，都是限定参数范围。但是所采用的距离定义不同。 我们很容易听说L1正则化能带来稀疏性，具体原因是，L1正则化很容易，在坐标轴上取得最优解，换成w即很容易让部分w置0，换成特征即，特征解耦，只判断某一部分特征便出结果，所以增加了稀疏性。 而去耦合的过程，也恰恰是减少过拟合的过程。 所以总结下来：L1与L2都能限定w范围，但是L1还能增加稀疏性。 l2正则化 权重衰退控制权重在一定范围，缩小参数范围，对参数值进行约束。 丢弃法随机按一定比例，选择某些节点置零，增加其他节点的大小，但是总体期望保持不变，因此增加了数据的噪音，从而防止过拟合。 BN批量归一化 训练时，只要网络前面几层发生一点点变化，到后面几层就会引起剧烈变化。所以我们使用批量归一化，控制每一层参数的分布，统一在一定大概范围，使得靠近输入层的参数，更新的稳定一点，这样上层更新参数时要做的变化就不会特别剧烈，训练起来就要稳定容易一些，训练复杂度也会降低。 同时，批量归一化后的分布，参数在进行学习的过程，是有回到原本分布的途径的，所以一定程度恢复了参数分布的原始表达能力。同时还有参数γ和β，不断学习，使得参数不断逼近最优分布。 批量归一化也可以看做加入了噪音，因为均值和方差，都是在当前批次下求出来的，所以每一小批次减除的都不同。于是增加了噪音，提高了模型的泛化能力。也是因此，我们很少将同为正则化的dropout与BN同时使用。一般会优先使用BN，因其效果更好。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://wxqiye.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://wxqiye.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]}],"categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://wxqiye.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"深度学习实践","slug":"深度学习实践","permalink":"https://wxqiye.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/"},{"name":"深度学习疑问与解答","slug":"深度学习疑问与解答","permalink":"https://wxqiye.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%96%91%E9%97%AE%E4%B8%8E%E8%A7%A3%E7%AD%94/"},{"name":"高中竞赛回忆","slug":"高中竞赛回忆","permalink":"https://wxqiye.github.io/categories/%E9%AB%98%E4%B8%AD%E7%AB%9E%E8%B5%9B%E5%9B%9E%E5%BF%86/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://wxqiye.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"深度学习实践","slug":"深度学习实践","permalink":"https://wxqiye.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/"},{"name":"深度学习疑问与解答","slug":"深度学习疑问与解答","permalink":"https://wxqiye.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%96%91%E9%97%AE%E4%B8%8E%E8%A7%A3%E7%AD%94/"},{"name":"高中竞赛回忆","slug":"高中竞赛回忆","permalink":"https://wxqiye.github.io/tags/%E9%AB%98%E4%B8%AD%E7%AB%9E%E8%B5%9B%E5%9B%9E%E5%BF%86/"}]}